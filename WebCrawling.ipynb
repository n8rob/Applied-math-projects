{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from book corpus\n",
    "Function below gathers the price for each fiction book and\n",
    "returns the mean price, in Â£, of a fiction book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books(start_page = \"index.html\"):\n",
    "    \"\"\" Crawl through http://books.toscrape.com and extract fiction data\"\"\"\n",
    "    base_url=\"http://books.toscrape.com/catalogue/category/books/fiction_10/\"\n",
    "    prices = []\n",
    "    page = base_url + start_page                # Complete page URL.\n",
    "    next_page_finder = re.compile(r\"next\")      # We need this button.\n",
    "    \n",
    "    current = None\n",
    "\n",
    "    for _ in range(2):\n",
    "        while current == None:                   # Try downloading until it works.\n",
    "            # Download the page source and PAUSE before continuing.  \n",
    "            page_source = requests.get(page).text\n",
    "            time.sleep(1)           # PAUSE before continuing.\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            current = soup.find_all(class_=\"product_pod\")\n",
    "            \n",
    "        # Navigate to the correct tag and extract title.\n",
    "        for book in current:\n",
    "            prices.append(float(book.h3.next_sibling.next_sibling.p.string[2:]))\n",
    "    \n",
    "        # ind the URL for the page with the next data\n",
    "        if \"page-2\" not in page:\n",
    "            # Find the URL for the page with the next data.\n",
    "            new_page = soup.find(string=next_page_finder).parent[\"href\"]    \n",
    "            page = base_url + new_page      # New complete page URL.\n",
    "            current = None\n",
    "    return sum(prices)/len(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping bank metadata\n",
    "Function below extracts the total consolidated assets (\"Consol\n",
    "Assets\") for JPMorgan Chase, Bank of America, and Wells Fargo recorded each December from\n",
    "2004 to the present. Returns a list of lists where each list contains the assets of each bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_data():\n",
    "    \"\"\"Crawl through the Federal Reserve site and extract bank data.\"\"\"\n",
    "    # Compile regular expressions for finding certain tags.\n",
    "    link_finder = re.compile(r\"December 31, (?!2003)\")\n",
    "    chase_bank_finder = re.compile(r\"^JPMORGAN CHASE BK\")\n",
    "    boa_bank_finder = re.compile(r\"^BANK OF AMER\")\n",
    "    wf_bank_finder = re.compile(r\"^WELLS FARGO BK\")\n",
    "\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.federalreserve.gov/releases/lbr/\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    link_tags = base_soup.find_all(name='a', href=True, string=link_finder)\n",
    "    pages = [base_url + tag.attrs[\"href\"] for tag in link_tags]\n",
    "\n",
    "    # Crawl through the individual pages and record the data.\n",
    "    chase_assets = []\n",
    "    boa_assets = [] \n",
    "    wf_assets = []\n",
    "    for page in pages:\n",
    "        time.sleep(1)               # PAUSE, then request the page.\n",
    "        soup = BeautifulSoup(requests.get(page).text, \"html.parser\")\n",
    "\n",
    "        # Find the tag corresponding to Chase banks' consolidated assets.\n",
    "        temp_tag = soup.find(name=\"td\", string=chase_bank_finder)\n",
    "\n",
    "        for _ in range(10):\n",
    "            temp_tag = temp_tag.next_sibling\n",
    "            \n",
    "        # Extract the data, removing commas.\n",
    "        chase_assets.append(int(temp_tag.string.replace(',', '')))\n",
    "        \n",
    "        # Find the tag corresponding to Bank of America banks' consolidated assets.\n",
    "        temp_tag = soup.find(name=\"td\", string=boa_bank_finder)\n",
    "\n",
    "        for _ in range(10):\n",
    "            temp_tag = temp_tag.next_sibling\n",
    "            \n",
    "        # Extract the data, removing commas.\n",
    "        boa_assets.append(int(temp_tag.string.replace(',', '')))\n",
    "        \n",
    "        # Find the tag corresponding to Wells Fargo banks' consolidated assets.\n",
    "        temp_tag = soup.find(name=\"td\", string=wf_bank_finder)\n",
    "\n",
    "        for _ in range(10):\n",
    "            temp_tag = temp_tag.next_sibling\n",
    "            \n",
    "        # Extract the data, removing commas.\n",
    "        wf_assets.append(int(temp_tag.string.replace(',', '')))\n",
    "\n",
    "    return [chase_assets, boa_assets, wf_assets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling data about basketball\n",
    "The Basketball Reference website at `https://www.basketball-reference.com`\n",
    "contains data on NBA athletes, including which player led different categories for each season.\n",
    "For the past ten seasons, we identify which player had the most season points and find how many\n",
    "points they scored during that season. Function below returns a list of triples consisting of the season, the\n",
    "player, and the points scored, (\"season year\", \"player name\", points scored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basketball_data():\n",
    "    '''The Basketball Reference website at \n",
    "    https://www.basketball-reference.com} hosts data on NBA athletes, \n",
    "    including which player led different categories.\n",
    "    For the past ten years, we identify which player had the most season points.\n",
    "    Return a list of triples, (\"season year\", \"player name\", points scored).\n",
    "    '''\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.basketball-reference.com\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    \n",
    "    # Now let's make the tags to look for \n",
    "    value_tags = [] \n",
    "    for i in range(10):\n",
    "        value_tags.append( \"/leagues/NBA_201\"+str(i)+\"_leaders.html\" )\n",
    "    # Get the href's from the soup\n",
    "    link_names = [list(base_soup.find_all(value=val_tag))[0][\"value\"] for val_tag in value_tags]\n",
    "    # Here's where we want to go \n",
    "    pages = [base_url + link_name for link_name in link_names]\n",
    "    \n",
    "    # Crawl through the individual pages and record the data. We want a triple as described above from each page\n",
    "    triples = []\n",
    "    for year_digit, page in enumerate(pages):\n",
    "        time.sleep(1)               # PAUSE, then request the page.\n",
    "        soup = BeautifulSoup(requests.get(page).text, \"html.parser\")\n",
    "        \n",
    "        # We have a soup. Get that triple!\n",
    "        player_name = list(soup.find(string=\"Points\").parent.next_sibling.next_sibling.children)[3].a.text\n",
    "        score = list(soup.find(string=\"Points\").parent.next_sibling.next_sibling.children)[5].text.strip()\n",
    "        year = 2010 + year_digit\n",
    "        season_string = str(year-1) + \"-\" + str(year)[-2:]\n",
    "        # we got it \n",
    "        triples.append((season_string, player_name, int(score)))\n",
    "    \n",
    "    return triples\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie data\n",
    "The website IMDB contains a variety of information on movies. Specifically,\n",
    "information on the top 10 box offce movies of the week can be found at `https://www.imdb.\n",
    "com/chart/boxoffice`. Using `BeautifulSoup` the function below returns a list of the top 10\n",
    "movies of the week and order the list according to the total grossing of the movies, from most\n",
    "money to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_data():\n",
    "    \"\"\"\n",
    "    Sort the Top 10 movies of the week by Total Grossing, taken from \n",
    "    https://www.imdb.com/chart/boxoffice?ref_=nv_ch_cht.\n",
    "\n",
    "    Returns:\n",
    "        titles (list): Top 10 movies of the week sorted by total grossing\n",
    "    \"\"\"\n",
    "    # We want to collect names and prices \n",
    "    mov_names = []\n",
    "    mov_prices = []\n",
    "    # Open the website\n",
    "    url=\"https://www.imdb.com/chart/boxoffice\"\n",
    "    page_source = requests.get(url).text\n",
    "    # Make some soup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    for movie_tag in soup.find_all(height=\"67\"):\n",
    "        # Get name and price\n",
    "        name = movie_tag.parent.next_sibling.next_sibling.a.text.strip()\n",
    "        mov_names.append(name)\n",
    "        price = float(movie_tag.parent.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.span.text.strip().strip('$').strip('M'))\n",
    "        mov_prices.append(price)\n",
    "    # Now our lists are all good\n",
    "    mov_names, mov_prices = np.array(mov_names), np.array(mov_prices)\n",
    "    order = np.argsort(mov_prices)[::-1]\n",
    "    return list(mov_names[order])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching arXiv with selenium\n",
    "### Ensure you have up-to-date web-driver\n",
    "The arXiv (pronounced \"archive\") is an online repository of scientific publications,\n",
    "hosted by Cornell University. Write a function that accepts a string to serve as a search\n",
    "query defaulting to linkedin. Use `Selenium` to enter the query into the search bar of `https:\n",
    "//arxiv.org` and press Enter. The resulting page has up to 50 links to the PDFs of technical\n",
    "papers that match the query. Gather these URLs, then continue to the next page (if there are\n",
    "more results) and continue gathering links until obtaining at most 150 URLs. Return the list\n",
    "of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob5(search_query=\"linkedin\"):\n",
    "    \"\"\"Use Selenium to enter the given search query into the search bar of\n",
    "    https://arxiv.org and press Enter. The resulting page has up to 25 links\n",
    "    to the PDFs of technical papers that match the query. Gather these URLs,\n",
    "    then continue to the next page (if there are more results) and continue\n",
    "    gathering links until obtaining at most 100 URLs. Return the list of URLs.\n",
    "\n",
    "    Returns:\n",
    "        (list): Up to 100 URLs that lead directly to PDFs on arXiv.\n",
    "    \"\"\"\n",
    "    browser = webdriver.Chrome()\n",
    "    try:\n",
    "        browser.get(\"https://arxiv.org\")\n",
    "        # Go to the search bar \n",
    "        try:\n",
    "            search_bar = browser.find_element_by_tag_name('input')\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(search_query)\n",
    "            # Now return using keys library\n",
    "            search_bar.send_keys(Keys.RETURN)\n",
    "        except NoSuchElementException:\n",
    "            print(\"Could not find the search bar!\")\n",
    "            raise\n",
    "        # Now we gotta change the number of results to 200\n",
    "        option = browser.find_element_by_xpath(\"//option[@value='200']\")\n",
    "        option.click()\n",
    "        go = browser.find_element_by_xpath(\"//button[@class='button is-small is-link']\")\n",
    "        go.click()\n",
    "        # Now we want to add things to the list of urls \n",
    "        list_urls = []\n",
    "        link_regex = re.compile(r'arXiv:\\d+?')\n",
    "        try:\n",
    "            page_source = requests.get(browser.current_url).text\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            for a in soup.find_all(string=link_regex,href=True):\n",
    "                list_urls.append(a[\"href\"])\n",
    "                if len(list_urls) > 100:\n",
    "                    break\n",
    "                # Now find the 'Next' button and push it!!!\n",
    "        except:\n",
    "            raise\n",
    "                \n",
    "    finally:\n",
    "        browser.close()\n",
    "    \n",
    "    return list_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape church-member-pertinent data from congregation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ward_list():\n",
    "    \n",
    "    # We want to collect names\n",
    "    ward_names = []\n",
    "    # Open the website\n",
    "    url=\"https://directory.churchofjesuschrist.org/169633\"\n",
    "    page_source = requests.get(url).text\n",
    "    # Make some soup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    for movie_tag in soup.find_all(height=\"67\"):\n",
    "        # Get name and price\n",
    "        name = movie_tag.parent.next_sibling.next_sibling.a.text.strip()\n",
    "        mov_names.append(name)\n",
    "        price = float(movie_tag.parent.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.span.text.strip().strip('$').strip('M'))\n",
    "        mov_prices.append(price)\n",
    "    # Now our lists are all good\n",
    "    mov_names, mov_prices = np.array(mov_names), np.array(mov_prices)\n",
    "    order = np.argsort(mov_prices)[::-1]\n",
    "    return list(mov_names[order])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
