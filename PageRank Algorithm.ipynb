{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nate's PageRank algorithm\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "import networkx as nx\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for representing directed graphs via their adjacency matrices...\n",
    "- We calculate $\\hat{A}$ as follows: $$\\hat{A}_{ij} = \\frac{\\tilde{A}_{ij}}{\\sum_{k=1} \\tilde{A}_{kj}} $$\n",
    "where $\\tilde{A}$ is $A$ (the adjacency matrix) with all \"sinks\" (columns of zeros) replaced as columns of ones.\n",
    "- Then we find the PageRank vector $\\textbf{p}$ in three different ways:\n",
    "- We solve the linear system $$(I-\\epsilon\\hat{A})\\textbf{p}=\\frac{1-\\epsilon}{n}\\textbf{1}$$\n",
    "- We let $E$ be a matrix of ones and define $B=\\epsilon\\hat{A}+\\frac{1-\\epsilon}{n}E$. Then $\\textbf{p}$ is an eigenvector of $B$ corresponding to the largest eigenvalue, $\\lambda=1$.\n",
    "- We pick an initial $\\textbf{p}(0)$ and iteratively solve with $$\\textbf{p}(t+1) = \\epsilon\\hat{A}\\textbf{p}(t) + \\frac{1-\\epsilon}{n}\\textbf{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiGraph:\n",
    "    \"\"\"A class for representing directed graphs via their adjacency matrices.\n",
    "\n",
    "    Attributes:\n",
    "        (fill this out after completing DiGraph.__init__().)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, A, labels=None):\n",
    "        \"\"\"Modify A so that there are no sinks in the corresponding graph. \n",
    "        (Sinks are columns of zeros, representing pages that don't link anywhere.)\n",
    "        Then calculate Ahat. Save Ahat and the labels as attributes.\n",
    "\n",
    "        Parameters:\n",
    "            A ((n,n) ndarray): the adjacency matrix of a directed graph.\n",
    "                A[i,j] is the weight of the edge from node j to node i.\n",
    "            labels (list(str)): labels for the n nodes in the graph.\n",
    "                If None, defaults to [0, 1, ..., n-1].\n",
    "        \"\"\"\n",
    "        #Check for zeros to see if we need to change them\n",
    "        num_rows,num_cols = A.shape\n",
    "        for i in range(num_cols):\n",
    "            if np.allclose(A[:,i], np.zeros(num_rows)):\n",
    "                A[:,i] = np.ones(num_rows)\n",
    "        #Now A is A_tilda\n",
    "        #We apply the above equation to make A_hat\n",
    "        big_sum_vector = np.array([sum(A[k,j] for k in range(num_rows)) for j in range(num_cols)])\n",
    "        A = A / big_sum_vector\n",
    "        #Now A is A_hat, so we store it as an attribute\n",
    "        self.A_hat = A\n",
    "\n",
    "        #Now what are the labels?\n",
    "        if labels is None:\n",
    "            labels = np.array(list(range(num_cols)))\n",
    "        else:\n",
    "            if len(labels) != num_cols:\n",
    "                raise ValueError(\"Number of labels must equal number of nodes.\")\n",
    "\n",
    "        #Store this information as attributes\n",
    "        self.labels = labels\n",
    "        self.num_nodes = num_cols\n",
    "\n",
    "    #    We will solve for the PageRank vector p three different ways...\n",
    "    \n",
    "    def linsolve(self, epsilon=0.85):\n",
    "        \"\"\"Compute the PageRank vector using the linear system method.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "        Returns:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #Get p\n",
    "        p = la.solve((np.eye(self.num_nodes) - epsilon*self.A_hat), ((1-epsilon)/self.num_nodes) * np.ones(self.num_nodes) )\n",
    "        #Normalize p\n",
    "        p = p/la.norm(p,ord=1)\n",
    "        #Get the dictionary mapping labels to the PageRank\n",
    "        keys = list(self.labels)\n",
    "        values = list(p)\n",
    "        dictionarp = dict(zip(keys, values))\n",
    "\n",
    "        return dictionarp\n",
    "\n",
    "    def eigensolve(self, epsilon=0.85):\n",
    "        \"\"\"Compute the PageRank vector using the eigenvalue method.\n",
    "        Normalize the resulting eigenvector so its entries sum to 1.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "        Return:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #Get B\n",
    "        B = epsilon*self.A_hat + ((1-epsilon)/self.num_nodes)*np.ones((self.num_nodes,self.num_nodes))\n",
    "        #Find first eignenvector of B\n",
    "        vals,vecs = la.eig(B)[0],la.eig(B)[1]\n",
    "        p = vecs[:,np.argmax(vals)]\n",
    "        #Scale it!\n",
    "        p = p / la.norm(p,ord=1)\n",
    "\n",
    "        #Get the dictionary mapping labels to the PageRank\n",
    "        keys = list(self.labels)\n",
    "        values = list(p)\n",
    "        dictionarp = dict(zip(keys, values))\n",
    "\n",
    "        return dictionarp\n",
    "\n",
    "    def itersolve(self, epsilon=0.85, maxiter=100, tol=1e-12):\n",
    "        \"\"\"Compute the PageRank vector using the iterative method.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "            maxiter (int): the maximum number of iterations to compute.\n",
    "            tol (float): the convergence tolerance.\n",
    "\n",
    "        Return:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #First get p0\n",
    "        p0 = (1/self.num_nodes)*np.ones(self.num_nodes)\n",
    "        one_vec = np.ones(self.num_nodes)\n",
    "\n",
    "        #Now we iterate\n",
    "        for t in range(maxiter):\n",
    "            #Get p1\n",
    "            p1 = epsilon*self.A_hat@p0 + ((1-epsilon)/self.num_nodes)*one_vec\n",
    "            #Check if we need to STOP!\n",
    "            if la.norm(p1-p0,ord=1) < tol:\n",
    "                break\n",
    "            #Now start over\n",
    "            p0 = p1\n",
    "\n",
    "        #Get the dictionary mapping labels to the PageRank\n",
    "        keys = list(self.labels)\n",
    "        values = list(p1)\n",
    "        dictionarp = dict(zip(keys, values))\n",
    "\n",
    "        return dictionarp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results to make sure they're the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly solving: {0: 0.09575863576738085, 1: 0.27415828596414515, 2: 0.35592479230432883, 3: 0.27415828596414515}\n",
      "Eigensolving: {0: 0.09575863576738075, 1: 0.2741582859641452, 2: 0.3559247923043289, 3: 0.2741582859641452}\n",
      "Iterative method: {0: 0.09575863576740319, 1: 0.27415828596408354, 2: 0.35592479230442975, 3: 0.27415828596408354}\n",
      "They are the same!\n"
     ]
    }
   ],
   "source": [
    "#Let's test...\n",
    "A = np.array([[0,0,0,0],[1,0,1,0],[1,0,0,1],[1,0,1,0]])\n",
    "epsilon = .85\n",
    "DG = DiGraph(A)\n",
    "\n",
    "#Solve\n",
    "print(\"Linearly solving:\",str(DG.linsolve(epsilon)))\n",
    "print(\"Eigensolving:\",str(DG.eigensolve(epsilon)))\n",
    "print(\"Iterative method:\",str(DG.itersolve(epsilon)))\n",
    "print(\"They are the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next funciton uses the code I wrote above to return a ranked list of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(d):\n",
    "    \"\"\"Construct a sorted list of labels based on the PageRank vector.\n",
    "\n",
    "    Parameters:\n",
    "        d (dict(str -> float)): a dictionary mapping labels to PageRank values.\n",
    "\n",
    "    Returns:\n",
    "        (list) the keys of d, sorted by PageRank value from greatest to least.\n",
    "    \"\"\"\n",
    "    #Get lists\n",
    "    labels = list(d.keys())\n",
    "    ranks = list(d.values())\n",
    "    #Now get the order\n",
    "    ord = list(np.argsort(ranks))[::-1]\n",
    "    #Sort that new list\n",
    "    labels = np.array(labels)\n",
    "    sorted_labels = labels[ord]\n",
    "    return list(sorted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example from above continued...\n",
    "get_ranks(DG.itersolve(epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file web_stanford.txt contains information on Stanford University webpages and the hyperlinks between them, gathered in 2002. This function gives us a ranked list of the webpages. (Note the labels in the file are webpage IDs, so they'll be nasty-looking numbers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_websites(filename=\"web_stanford.txt\", epsilon=0.85):\n",
    "    \"\"\"Read the specified file and construct a graph where node j points to\n",
    "    node i if webpage j has a hyperlink to webpage i. Use the DiGraph class\n",
    "    and its itersolve() method to compute the PageRank values of the webpages,\n",
    "    then rank them with get_ranks().\n",
    "\n",
    "    Each line of the file has the format\n",
    "        a/b/c/d/e/f...\n",
    "    meaning the webpage with ID 'a' has hyperlinks to the webpages with IDs\n",
    "    'b', 'c', 'd', and so on.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): the file to read from.\n",
    "        epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        (list(str)): The ranked list of webpage IDs.\n",
    "    \"\"\"\n",
    "    #read in the data\n",
    "    with open(filename,'r') as myfile:\n",
    "        data = myfile.readlines()\n",
    "    #data is a list of strings\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].strip().split('/')\n",
    "    #Now data is a list of lists, lol\n",
    "    unique_nodes = set(item[i] for item in data for i in range(len(item)))\n",
    "    #Make an alphabetical list\n",
    "    ordered_nodes = list(unique_nodes)\n",
    "    ordered_nodes.sort()\n",
    "    #make a dictionary\n",
    "    num = len(ordered_nodes)\n",
    "    nodes_to_numbers = dict(zip(ordered_nodes, list(range(num))))\n",
    "    #Make a blank matrix\n",
    "    M = np.zeros((num, num))\n",
    "    #Now we do a CRAZY LOOP\n",
    "    for i in range(len(data)):\n",
    "        #What row are we looking at\n",
    "        row_in_question = data[i]\n",
    "        #Save number of first entry\n",
    "        i_matrix = nodes_to_numbers[row_in_question[0]]\n",
    "        for j in range(1,len(row_in_question)):\n",
    "            datum_in_question = row_in_question[j]\n",
    "            #Save number of entry\n",
    "            j_matrix = nodes_to_numbers[datum_in_question]\n",
    "            #Update matrix\n",
    "            M[j_matrix,i_matrix] = 1\n",
    "    #call the class!\n",
    "    DGM = DiGraph(M,labels=ordered_nodes)\n",
    "    ranks_dict = DGM.itersolve(epsilon=epsilon)\n",
    "\n",
    "    return get_ranks(ranks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98595',\n",
       " '32791',\n",
       " '28392',\n",
       " '77323',\n",
       " '92715',\n",
       " '26083',\n",
       " '130094',\n",
       " '99464',\n",
       " '12846',\n",
       " '106064',\n",
       " '332',\n",
       " '31328',\n",
       " '86049',\n",
       " '123900',\n",
       " '74923']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do it! (for first 15 websites)\n",
    "rank_websites()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use this same strategy to rank basketball teams in a given year.\n",
    "Files to use:\n",
    "- ncaa2014.csv\n",
    "- ncaa2015.csv\n",
    "- ncaa2016.csv\n",
    "- ncaa2017.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_ncaa_teams(filename, epsilon=0.85):\n",
    "    \"\"\"Read the specified file and construct a graph where node j points to\n",
    "    node i with weight w if team j was defeated by team i in w games. Use the\n",
    "    DiGraph class and its itersolve() method to compute the PageRank values of\n",
    "    the teams, then rank them with get_ranks().\n",
    "\n",
    "    Each line of the file has the format\n",
    "        A,B\n",
    "    meaning team A defeated team B.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): the name of the data file to read.\n",
    "        epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        (list(str)): The ranked list of team names.\n",
    "    \"\"\"\n",
    "    #read in the data\n",
    "    with open(filename,'r') as myfile:\n",
    "        data = myfile.readlines()\n",
    "    #data is a list of strings\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].strip().split(',')\n",
    "    #Now data is a list of lists, lol\n",
    "    #Get rid of the header\n",
    "    data = data[1:]\n",
    "    #We want a list of unique teams\n",
    "    unique_teams_set = set(item[i] for item in data for i in range(2))\n",
    "    unique_teams_list = list(unique_teams_set)\n",
    "    unique_teams_list.sort()\n",
    "    num_teams = len(unique_teams_list)\n",
    "    #And we want a dictionary associating each team with a number\n",
    "    teams_to_numbers = dict(zip(unique_teams_list, list(range(num_teams))))\n",
    "\n",
    "    #   Now we make the matrix representing a graph representing the wins and loses\n",
    "    #   Losers point to winners, path weight corresponds to number of losses\n",
    "    #   E.g. If team A lost to team B twice, we have A--(2)-->B\n",
    "    #   ...and the B,A entry of the matrix is equal to 2\n",
    "\n",
    "    M = np.zeros((num_teams,num_teams))\n",
    "    for i in range(len(data)):\n",
    "        winner_num = teams_to_numbers[data[i][0]]\n",
    "        loser_num = teams_to_numbers[data[i][1]]\n",
    "        M[winner_num,loser_num] += 1\n",
    "    #Now M is constructed\n",
    "    DGM = DiGraph(M,labels=unique_teams_list)\n",
    "    ranks_dict = DGM.itersolve(epsilon=epsilon)\n",
    "\n",
    "    return get_ranks(ranks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have some fun. :) First 15 teams each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Duke',\n",
       " 'Wisconsin',\n",
       " 'Notre Dame',\n",
       " 'North Carolina State',\n",
       " 'Virginia',\n",
       " 'Kentucky',\n",
       " 'Maryland',\n",
       " 'Louisville',\n",
       " 'Michigan State',\n",
       " 'UNC',\n",
       " 'Miami (FL)',\n",
       " 'Arizona',\n",
       " 'Villanova',\n",
       " 'Kansas',\n",
       " 'Providence']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_ncaa_teams('ncaa2014.csv')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Villanova',\n",
       " 'Virginia',\n",
       " 'Oklahoma',\n",
       " 'Kansas',\n",
       " 'UNC',\n",
       " 'Oregon',\n",
       " 'Xavier',\n",
       " 'Seton Hall',\n",
       " 'Michigan State',\n",
       " 'Miami (FL)',\n",
       " 'West Virginia',\n",
       " 'Iowa State',\n",
       " 'Duke',\n",
       " 'Texas',\n",
       " 'Wisconsin']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_ncaa_teams('ncaa2015.csv')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Georgetown',\n",
       " 'Illinois',\n",
       " 'Xavier',\n",
       " 'DePaul',\n",
       " \"St. John's (NY)\",\n",
       " 'Creighton',\n",
       " 'Maryland',\n",
       " 'Portland',\n",
       " 'Louisville',\n",
       " 'Pittsburgh',\n",
       " 'Marquette',\n",
       " 'UCLA',\n",
       " 'Florida St.',\n",
       " 'Providence',\n",
       " 'Villanova']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_ncaa_teams('ncaa2016.csv')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Villanova',\n",
       " 'Kansas',\n",
       " 'Providence',\n",
       " 'West Virginia',\n",
       " 'Michigan',\n",
       " 'Texas Tech',\n",
       " 'Virginia',\n",
       " 'UNC',\n",
       " 'Xavier',\n",
       " 'Butler',\n",
       " 'Florida',\n",
       " 'Duke',\n",
       " 'Kentucky',\n",
       " 'Tennessee',\n",
       " 'Purdue']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_ncaa_teams('ncaa2017.csv')[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rank actors. The directed graph here has nodes for actors linked to the node for the lead actor in any movie they were in. (E.g. Michael Cain $\\rightarrow$ Christian Bale in The Dark Knight Rises.) The most linked-to actors appear first in the ranked list. Note that the ranked list may be different with a different damping factor $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_actors(filename=\"top250movies.txt\", epsilon=0.85):\n",
    "    \"\"\"Read the specified file and construct a graph where node a points to\n",
    "    node b with weight w if actor a and actor b were in w movies together but\n",
    "    actor b was listed first. Use NetworkX to compute the PageRank values of\n",
    "    the actors, then rank them with get_ranks().\n",
    "\n",
    "    Each line of the file has the format\n",
    "        title/actor1/actor2/actor3/...\n",
    "    meaning actor2 and actor3 should each have an edge pointing to actor1,\n",
    "    and actor3 should have an edge pointing to actor2.\n",
    "    \"\"\"\n",
    "\n",
    "    #   Read in the data and turn it into a list of lists of actor names\n",
    "\n",
    "    #read in the data\n",
    "    with open(filename,'r',encoding=\"utf-8\") as myfile:\n",
    "        data = myfile.readlines()\n",
    "    #data is a list of strings\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].strip().split('/')[1:]\n",
    "    #Now data is a list of lists without movie titles\n",
    "\n",
    "    #   Now we convert each list in data into a list of combinations and\n",
    "    #   feed those into the DG (directed graph).\n",
    "    #   We want a graph with secondary actors pointing to primary actors with a\n",
    "    #   weight of how many movies they were in together (in that same order).\n",
    "\n",
    "    DG = nx.DiGraph()\n",
    "    for row in range(len(data)):\n",
    "        movie_in_question = data[row]\n",
    "        actor_combos = list(itertools.combinations(movie_in_question,2))\n",
    "        #^Note that actor_combos is a list of tuples, with the primary actor\n",
    "        #first in each tuple\n",
    "        for combo in actor_combos:\n",
    "            #Check if the edge is already there\n",
    "            if DG.has_edge(combo[1],combo[0]):\n",
    "                DG[combo[1]][combo[0]][\"weight\"] += 1\n",
    "            else:\n",
    "                DG.add_edge(combo[1],combo[0],weight=1)\n",
    "\n",
    "    #   The DG should be all constructed now\n",
    "    #   We need to get the pagerank dictionary and then get a ranked list of\n",
    "    #   actors from the function from Problem 3\n",
    "\n",
    "    pr_dict = nx.pagerank(DG,alpha=epsilon)\n",
    "    return get_ranks(pr_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out! Top 15 actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leonardo DiCaprio',\n",
       " 'Jamie Foxx',\n",
       " 'Christoph Waltz',\n",
       " 'Robert De Niro',\n",
       " 'Al Pacino',\n",
       " 'Tom Hanks',\n",
       " 'Antonella Attili',\n",
       " 'Brad Pitt',\n",
       " 'Christian Bale',\n",
       " 'Ben Kingsley',\n",
       " 'Diahnne Abbott',\n",
       " 'Liam Neeson',\n",
       " 'Ralph Fiennes',\n",
       " 'Matt Damon',\n",
       " 'Tom Hardy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_actors()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leonardo DiCaprio',\n",
       " 'Robert De Niro',\n",
       " 'Tom Hanks',\n",
       " 'Jamie Foxx',\n",
       " 'Al Pacino',\n",
       " 'Christoph Waltz',\n",
       " 'Christian Bale',\n",
       " 'Ben Kingsley',\n",
       " 'Ralph Fiennes',\n",
       " 'Matt Damon',\n",
       " 'Morgan Freeman',\n",
       " 'Tom Hardy',\n",
       " 'Liam Neeson',\n",
       " 'Brad Pitt',\n",
       " 'Gary Oldman']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_actors(epsilon=.7)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
